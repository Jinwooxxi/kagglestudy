{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic_3rd_kernel_김진우.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv/WADdq9Mp2nJYbVZV22t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinwooxxi/kagglestudy/blob/main/titanic/Titanic_3rd_kernel_%EA%B9%80%EC%A7%84%EC%9A%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I_Kr3GZHinJ"
      },
      "source": [
        "필사 참조:\n",
        "\n",
        "https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oDSAwhp5Tcj"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "The Titanic competition which is a good way to ingroduce feature engineering and ensemble modiling. Fistrly, display some featrue analyses then focous on the feature engineering. Last part concerns modeling and predicting the survival onthe Titanic using an voting procedure.\n",
        "\n",
        "This script follows three main parts:\n",
        "\n",
        "* Feature analysis\n",
        "* Featrue engineering\n",
        "* Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6vfw7D4-Or"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
        "\n",
        "sns.set(style='white', context='notebook', palette='deep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfaMrmSk5NAV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqgrdV_6A2C"
      },
      "source": [
        "# 2. Load and check data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwWkNeEH6Fka"
      },
      "source": [
        "## 2.1 Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ_lq_Nz5M9f"
      },
      "source": [
        "# Load data\n",
        "## Load train and test set\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/titanic/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/titanic/test.csv\")\n",
        "IDtest = test[\"PassengerId\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UN7N-4o6VMG"
      },
      "source": [
        "## 2.2 Outlier detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkyZy-B5M61"
      },
      "source": [
        "def detect_ouliers(df, n, features):\n",
        "  \"\"\"\n",
        "  Takes a dataframe df of features and retruns a list of the indices corresponding to the observatrions\n",
        "  containing more than n ouliers according to the Tukey mechod.\n",
        "  \"\"\"\n",
        "  outlier_indices = []\n",
        "\n",
        "  # iterate over features(columns)\n",
        "  for col in features:\n",
        "    # 1st quartile (25%)\n",
        "    Q1 = np.percentile(df[col], 25)\n",
        "    # 3rd quartile (75%)\n",
        "    Q3 = np.percentile(df[col], 75)\n",
        "    # Interquartile range (IQR)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # outlier step\n",
        "    outlier_step = 1.5 * IQR\n",
        "\n",
        "    # Determine a list of indices of outliers for feature col\n",
        "    outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
        "\n",
        "    # append the fount outlier indices for col to the list of outlier indiced\n",
        "    outlier_indices.extend(outlier_list_col)\n",
        "  \n",
        "  # select observations containing more than 2 outliers\n",
        "  outlier_indices = Counter(outlier_indices)\n",
        "  multiple_outliers = list(k for k, v in outlier_indices.items() if v >n)\n",
        "\n",
        "  return multiple_outliers\n",
        "\n",
        "# detect ouliers from Age, SibSP, Parch and Fare\n",
        "Outliers_to_drop = detect_ouliers(train, 2, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q3_5ZEi8ShS"
      },
      "source": [
        "Since outliers can have a dramatic effect on the prediction(expacially for regression problems).\n",
        "\n",
        "The Tukey method (Tukey JW., 1977) to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n",
        "\n",
        "Detect outliers from the numerical values features (Age, SibSp, Sarch and Fare). Then, considered outliers as rows that have at least two outlied numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnyWROp5M1h"
      },
      "source": [
        "train.loc[Outliers_to_drop] # show the outliers rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFxv7Qd9USb"
      },
      "source": [
        "Detect 10 outliers. The 28, 89 and 342 passenger have an high Ticket Fare\n",
        "\n",
        "The 7 others have very high values of SibSP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJzhVD75MzH"
      },
      "source": [
        "# Drop Outliers\n",
        "train = train.drop(Outliers_to_drop, axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geSjhWNX9jSE"
      },
      "source": [
        "## 2.3 Joining train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE0fuUvY5Mwb"
      },
      "source": [
        "# Join train and test dateasets in order to obtain the same number of features during categorical conversion\n",
        "train_len = len(train)\n",
        "dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0PtkUmd-DIU"
      },
      "source": [
        "## 2.4 Check for null and missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4JB4SfK5MuK"
      },
      "source": [
        "# Fill empy and NaNs values with Nan\n",
        "dataset = dataset.fillna(np.nan)\n",
        "\n",
        "# Check for Null Values\n",
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3vgpCLX-W_v"
      },
      "source": [
        "Age and Cabin features have an important part of missing values.\n",
        "\n",
        "***Survived missing values correspond to the join testing dataset(Survived column doesn'n exist in test set and has been replace by NaN values when concatenating the train and test set)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dcAec6j5MrW"
      },
      "source": [
        "# Infos\n",
        "train.info()\n",
        "print()\n",
        "train.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgUA3P7l5Moz"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZpR--Jb-9ak"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRkq6L1-9YH"
      },
      "source": [
        "### Summarize data\n",
        "# Summarize and statistics\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0RBllOH_JNn"
      },
      "source": [
        "# 3. Feature analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqptZwk6_L2t"
      },
      "source": [
        "## 3.1 Numerical values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhxuuQB4-9Ve"
      },
      "source": [
        "# Corrlation matrix between numerical values (SibSp, Parch, Age, and Fare values) and Survived\n",
        "g = sns.heatmap(train[[\"Survived\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"]].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUJxNf0_y7M"
      },
      "source": [
        "Only Fare feature seems to have a significative correlation with the survival probability.\n",
        "\n",
        "It doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhYXMpxp_12_"
      },
      "source": [
        "### SibSp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhauh9tg-9Sz"
      },
      "source": [
        "# Explore SibSp feature vs Survived\n",
        "g = sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRytQhR8AgUw"
      },
      "source": [
        "It seems that passengers having a lot of siblings/spouses have less chance to survive.\n",
        "\n",
        "Single passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive.\n",
        "\n",
        "This observation is quite interesting, we can consider a new feature describing these categories (See feature engineering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdZSxdXMAl8L"
      },
      "source": [
        "### Parch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--rzQaM1-9QL"
      },
      "source": [
        "# Explor Parch feature vs Survived\n",
        "g = sns.factorplot(x=\"Parch\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6gR0Xh9A1Bf"
      },
      "source": [
        "Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n",
        "\n",
        "Be carefull there is an important standard deviation in the survival of passengers with 3 parents/children."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQEarPR_A360"
      },
      "source": [
        "### Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_XB8p1C-9Nf"
      },
      "source": [
        "# Explore Age vs Survived\n",
        "g = sns.FacetGrid(train, col=\"Survived\")\n",
        "g = g.map(sns.distplot, \"Age\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcQ7P5kpBQrI"
      },
      "source": [
        "Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n",
        "\n",
        "We notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived.\n",
        "\n",
        "So, even if \"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive.\n",
        "\n",
        "It seems that very young passengers have more chance to survive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlcO5DJm-9LY"
      },
      "source": [
        "# Explore Age Distribution\n",
        "g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade=True)\n",
        "g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax=g, color=\"Blue\", shade=True)\n",
        "g.set_xlabel(\"Age\")\n",
        "g.set_ylabel(\"Frequency\")\n",
        "g = g.legend([\"Not Survived\", \"Survived\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPUaCQp1B9Je"
      },
      "source": [
        "When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLwyGf06B-tg"
      },
      "source": [
        "### Fare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSXjhru-9Id"
      },
      "source": [
        "dataset[\"Fare\"].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNGq9pv7-9F7"
      },
      "source": [
        "# Fill Fare missing values with the medin value\n",
        "dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pow__A7CUyL"
      },
      "source": [
        "Since we have one missing value , decided to fill it with the median value which will not have an important effect on the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApLRm7zv-9DQ"
      },
      "source": [
        "# Explore face distribution\n",
        "g = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\n",
        "g = g.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U7xDLg8Cz-v"
      },
      "source": [
        "As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n",
        "\n",
        "In this case, it is better to transform it with the log function to reduce this skew.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWEJZxWt-9A3"
      },
      "source": [
        "# Apply log to Fare to reduce skewness distribution\n",
        "dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i:np.log(i) if i>0 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGgDVZTw-8-S"
      },
      "source": [
        "g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\n",
        "g = g.legend(loc=\"best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3BSi2tEDVpK"
      },
      "source": [
        "Skewness is clearly reduced after the log transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj03qJ8ZKYQr"
      },
      "source": [
        "## 3.2 Categorical values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POaGee8BKcOB"
      },
      "source": [
        "### Sex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSZJw6hv-87l"
      },
      "source": [
        "g = sns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n",
        "g = g.set_ylabel(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ4vkVcg-85D"
      },
      "source": [
        "train[[\"Sex\", \"Survived\"]].groupby(\"Sex\").mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkzIl1M8K0p7"
      },
      "source": [
        "It is clearly obvious that Male have less chance to survive than Female.\n",
        "\n",
        "So Sex, might play an important role in the prediction of the survival."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsFUom8mK192"
      },
      "source": [
        "### Pclass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8us5nvJ-82j"
      },
      "source": [
        "# Expolore Pclass vs Survived\n",
        "g = sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train, kind=\"bar\", size=6, palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dolWP59R-8z8"
      },
      "source": [
        "# Explore Pclass vs Survived by Sex\n",
        "g = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, size=6, kind=\"bar\", palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2LqOyU5L9IK"
      },
      "source": [
        "The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.\n",
        "\n",
        "This trend is conserved when we look at both male and female passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQh55qFBL-Xy"
      },
      "source": [
        "### Embarked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C6U-8kI-8xj"
      },
      "source": [
        "dataset[\"Embarked\"].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYi7SxZm-8u1"
      },
      "source": [
        "# Fill Embarked NaN value of dataset set with \"S\" most frequent value\n",
        "dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rqh-OYK-8sd"
      },
      "source": [
        "# Explore Embarked vs Survived \n",
        "g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train, size=6, kind=\"bar\", palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"survival probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFfn5OhyMiID"
      },
      "source": [
        "It seems that passenger coming from Cherbourg (C) have more chance to survive.\n",
        "\n",
        "Its hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n",
        "\n",
        "Let's see the Pclass distribution vs Embarked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLG90sf--8qI"
      },
      "source": [
        "# Explore Pclass vs Embarked \n",
        "g = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train, size=6, kind=\"count\", palette=\"muted\")\n",
        "g.despine(left=True)\n",
        "g = g.set_ylabels(\"Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6f7PLxmMyCD"
      },
      "source": [
        "Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n",
        "\n",
        "At this point, can't explain why first class has an higher survival rate. Its hypothesis is that first class passengers were prioritised during the evacuation due to their influence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWQecquoM2uF"
      },
      "source": [
        "# 4. Filling missing Values\n",
        "\n",
        "Age column contains 256 missing values in the whole dataset.\n",
        "\n",
        "Since there is subpopulations that have more chance to survived (children for example), it is preferable to keep the age feature and to impute the missing values.\n",
        "\n",
        "To adress this problem, looked at the most corrlated features with Age(Sex, Parch, Pclass, and SibSp)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK7-axsB-8n7"
      },
      "source": [
        "# Explore Age vs Sex, Parch , Pclass and SibSP\n",
        "g = sns.factorplot(y=\"Age\", x=\"Sex\", data=dataset, kind=\"box\")\n",
        "g = sns.factorplot(y=\"Age\", x=\"Sex\", hue=\"Pclass\", data=dataset, kind=\"box\")\n",
        "g = sns.factorplot(y=\"Age\", x=\"Parch\", data=dataset,kind=\"box\")\n",
        "g = sns.factorplot(y=\"Age\", x=\"SibSp\", data=dataset,kind=\"box\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3auAzPbNuHh"
      },
      "source": [
        "Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n",
        "\n",
        "However, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n",
        "\n",
        "Moreover, the more a passenger has parents/children the older he is and the more a passenger has siblings/spouses the younger he is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH1ri1rj-8lE"
      },
      "source": [
        "# convert Sex into categorical value 0 for male and 1 for female\n",
        "dataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\":0, \"female\":1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTPtxELO-8in"
      },
      "source": [
        "g = sns.heatmap(dataset[[\"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]].corr(), cmap=\"BrBG\", annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enr7yknUOWR9"
      },
      "source": [
        "The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n",
        "\n",
        "In the plot of Age in function of Parch, Age is growing with the number of parents / children. But the general correlation is negative.\n",
        "\n",
        "So, decided to use SibSP, Parch and Pclass in order to impute the missing ages.\n",
        "\n",
        "The strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoWgi158-8gQ"
      },
      "source": [
        "# Filling missing value of Age\n",
        "\n",
        "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
        "# Index of NaN age rows\n",
        "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isna()].index)\n",
        "\n",
        "for i in index_NaN_age:\n",
        "  age_med = dataset[\"Age\"].median()\n",
        "  age_pred = dataset[\"Age\"][((dataset[\"SibSp\"] == dataset.iloc[i][\"SibSp\"]) & \\\n",
        "                             (dataset[\"Parch\"] == dataset.iloc[i][\"Parch\"]) & \\\n",
        "                             (dataset[\"Pclass\"] == dataset.iloc[i][\"Pclass\"]))].median()\n",
        "  if not np.isnan(age_pred):\n",
        "    dataset[\"Age\"].iloc[i] = age_pred\n",
        "  else:\n",
        "    dataset[\"Age\"].iloc[i] = age_med"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3QMc52N-8eU"
      },
      "source": [
        "g = sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"box\")\n",
        "g = sns.factorplot(x=\"Survived\", y=\"Age\", data=train, kind=\"violin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yboq5PYyQDNN"
      },
      "source": [
        "No difference between median value of age in survived and not survived subpopulation.\n",
        "\n",
        "But in the violin plot of survived passengers, we still notice that very young passengers have higher survival rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6l4wNA8QEqa"
      },
      "source": [
        "# 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ERQARyQMr-"
      },
      "source": [
        "## 5.1 Name/Title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npadxiHw-8bl"
      },
      "source": [
        "dataset[\"Name\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAGJRIwGQbok"
      },
      "source": [
        "The Name feature contains information on passenger's title.\n",
        "\n",
        "Since some passenger with distingused title may be preferred during the evacuation, it is interesting to add them to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJEDnQgb-8ZE"
      },
      "source": [
        "# Get Title from Name\n",
        "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
        "dataset[\"Title\"] = pd.Series(dataset_title)\n",
        "dataset[\"Title\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl1QR24r-8W1"
      },
      "source": [
        "g = sns.countplot(x=\"Title\", data=dataset)\n",
        "g = plt.setp(g.get_xticklabels(), rotation=45) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWFZ5K37RB05"
      },
      "source": [
        "There is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRnhIK7z-8US"
      },
      "source": [
        "# Convert to categorical values Title\n",
        "dataset[\"Title\"] = dataset[\"Title\"].replace([\"Lady\", \"the Countess\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\",\n",
        "                                             \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
        "dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \n",
        "                                         \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
        "dataset[\"Title\"] = dataset[\"Title\"].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDAmalTE5Mjl"
      },
      "source": [
        "g = sns.countplot(dataset[\"Title\"])\n",
        "g = g.set_xticklabels([\"Master\", \"Miss/Ms/Mme/Mlle/Mrs\", \"Mr\", \"Rare\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42d0F_kI5MhI"
      },
      "source": [
        "g = sns.factorplot(x=\"Title\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
        "g = g.set_xticklabels([\"Master\", \"Miss-Mrs\", \"Mr\", \"Rare\"])\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLoAO_1V5McA"
      },
      "source": [
        "# Drop Name variable\n",
        "dataset.drop(labels=[\"Name\"], axis=1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFfIYk2PV77N"
      },
      "source": [
        "## 5.2 Family Size\n",
        "\n",
        "We can imagine that large families will have more difficulties to evacuate, looking for theirs sisters/brothers/parents during the evacuation. So, create a \"Fize\"(family size) feature which is the sum of SibSp,Parch and 1(including the passenger)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8187SFti5MZk"
      },
      "source": [
        "# Create a family size descriptor from SibSp and Parch\n",
        "dataset[\"Fsize\"] = dataset[\"Parch\"] + dataset[\"SibSp\"] + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVCiOC405MXE"
      },
      "source": [
        "g = sns.factorplot(x=\"Fsize\", y=\"Survived\", data=dataset)\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiTv4lzW2Bh"
      },
      "source": [
        "The family size seems to play an important role, survival probability is worst for large families.\n",
        "\n",
        "Additionally, decided to created 4 categories of family size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "try1EMYu5MUm"
      },
      "source": [
        "# Create new feature of family size\n",
        "dataset[\"Single\"] = dataset[\"Fsize\"].map(lambda s: 1 if s == 1 else 0)\n",
        "dataset[\"SmallF\"] = dataset[\"Fsize\"].map(lambda s: 1 if  s == 2  else 0)\n",
        "dataset[\"MedF\"] = dataset[\"Fsize\"].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
        "dataset[\"LargeF\"] = dataset[\"Fsize\"].map(lambda s: 1 if s >= 5 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BxZdAOJ5MSU"
      },
      "source": [
        "g = sns.factorplot(x=\"Single\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
        "g = g.set_ylabels(\"Survival Probability\")\n",
        "\n",
        "g = sns.factorplot(x=\"SmallF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
        "g = g.set_ylabels(\"Survival Probability\")\n",
        "\n",
        "g = sns.factorplot(x=\"MedF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
        "g = g.set_ylabels(\"Survival Probability\")\n",
        "\n",
        "g = sns.factorplot(x=\"LargeF\", y=\"Survived\", data=dataset, kind=\"bar\")\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CDiv5ZXUAP"
      },
      "source": [
        "Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPZlR8p75MPs"
      },
      "source": [
        "# convert to indicator values Title and Embarked \n",
        "dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n",
        "dataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWWTXJnh5MNQ"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQbpHwSLXr5K"
      },
      "source": [
        "## 5.3 Cabin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp1hL7G25MK9"
      },
      "source": [
        "dataset[\"Cabin\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BktZTuAe5MIU"
      },
      "source": [
        "dataset[\"Cabin\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtPGJLI45MFp"
      },
      "source": [
        "dataset[\"Cabin\"].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31M8RhisX2lt"
      },
      "source": [
        "The Cabin feature column contains 292 values and 1007 missing values.\n",
        "\n",
        "Supposed that passengers without a cabin have a missing value displayed instead of the cabin number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83UUjsahQJN-"
      },
      "source": [
        "dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9ST9MHQJKz"
      },
      "source": [
        "# Replace the Cabin number by the type of cabin \"X\" if not\n",
        "dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isna(i) else \"X\" for i in dataset[\"Cabin\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWTZfgWAYU1n"
      },
      "source": [
        "The first letter of the cabin indicates the Desk,  choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvtzvUPEQJGy"
      },
      "source": [
        "g = sns.countplot(dataset[\"Cabin\"], order=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"X\"]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoazsibkQJDm"
      },
      "source": [
        "g = sns.factorplot(y=\"Survived\", x=\"Cabin\", data=dataset, kind=\"bar\", order=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"X\"])\n",
        "g = g.set_ylabels(\"Survival Probability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yde-v5_ZNPv"
      },
      "source": [
        "Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks.\n",
        "\n",
        "But we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n",
        "\n",
        "It is particularly true for cabin B, C, D, E and F."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lDu0i-_QJAv"
      },
      "source": [
        "dataset = pd.get_dummies(dataset, columns=[\"Cabin\"], prefix=\"Cabin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDh5kWnFZT1_"
      },
      "source": [
        "## 5.4 Ticket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5wL0yG2QI93"
      },
      "source": [
        "dataset[\"Ticket\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q-FYuNfZ8Q6"
      },
      "source": [
        "It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therefore lead to the actual placement of the cabins within the ship.\n",
        "\n",
        "Tickets with same prefixes may have a similar class and survival.\n",
        "\n",
        "So replace the Ticket feature column by the ticket prefixe. Which may be more informative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFXhOMMmQI68"
      },
      "source": [
        "## Treat Ticket by extracting the ticket prefix. When there is no prefix it retrun X.\n",
        "\n",
        "Ticket = []\n",
        "for i in list(dataset.Ticket):\n",
        "  if not i.isdigit():\n",
        "    Ticket.append(i.replace(\".\", \"\").replace(\"/\",\"\").strip().split(\" \")[0])\n",
        "  # Take preifix\n",
        "  else:\n",
        "    Ticket.append(\"X\")\n",
        "\n",
        "dataset[\"Ticket\"] = Ticket\n",
        "dataset[\"Ticket\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxm3Vox1QI4B"
      },
      "source": [
        "dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY0tyro-QI0L"
      },
      "source": [
        "# Create categorical values for Pclass\n",
        "dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n",
        "dataset = pd.get_dummies(dataset, columns = [\"Pclass\"], prefix=\"Pc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moJtJoScQIwj"
      },
      "source": [
        "# Drop useless variables\n",
        "dataset.drop(labels=[\"PassengerId\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTs_ky0kQIty"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGA9g2YUQIpO"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeXCm__Wqlq"
      },
      "source": [
        "# 6. Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAgPXOyaWuwG"
      },
      "source": [
        "## Separate train dataset and test dataset\n",
        "\n",
        "train = dataset[:train_len]\n",
        "test = dataset[train_len:]\n",
        "test.drop(labels=[\"Survived\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rkUdMj0Wuta"
      },
      "source": [
        "## Seperate train features and label\n",
        "\n",
        "train[\"Survived\"] = train[\"Survived\"].astype(int)\n",
        "Y_train = train[\"Survived\"]\n",
        "X_train = train.drop(labels=[\"Survived\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h5itZ4UXqes"
      },
      "source": [
        "## 6.1 Simple modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIb2QlBFXtrS"
      },
      "source": [
        "### 6.1.1 Cross Validata models\n",
        "\n",
        "Compared 10 poupular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n",
        "\n",
        "* SVC\n",
        "* Decision Tres\n",
        "* Adaboost\n",
        "* Random Forest\n",
        "* Extra Tress\n",
        "* Gradient Boosting\n",
        "* Multiple layer perceptron(neural network)\n",
        "* KNN\n",
        "* Logistic regression\n",
        "* Linear Discriminant Anaysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDOePl5aWuq1"
      },
      "source": [
        "# Cross validate model with Kfold stratified cross val\n",
        "kfold = StratifiedKFold(n_splits=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC6T43jbWuoM"
      },
      "source": [
        "# Modeling step Test differents algorithms\n",
        "random_state = 2\n",
        "classifiers = []\n",
        "classifiers.append(SVC(random_state=random_state))\n",
        "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
        "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), \n",
        "                   random_state=random_state, learning_rate=0.1))\n",
        "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
        "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
        "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
        "classifiers.append(MLPClassifier(random_state=random_state))\n",
        "classifiers.append(KNeighborsClassifier())\n",
        "classifiers.append(LogisticRegression(random_state=random_state))\n",
        "classifiers.append(LinearDiscriminantAnalysis())\n",
        "\n",
        "cv_reuslts = []\n",
        "for classifier in classifiers:\n",
        "  cv_reuslts.append(cross_val_score(classifier, X_train, y=Y_train, scoring=\"accuracy\", cv=kfold, n_jobs=4))\n",
        "\n",
        "cv_means = []\n",
        "cv_std = []\n",
        "for cv_result in cv_reuslts:\n",
        "  cv_means.append(cv_result.mean())\n",
        "  cv_std.append(cv_result.std())\n",
        "\n",
        "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means, \"CrossValerrors\":cv_std,\n",
        "                       \"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\"RandomForest\",\n",
        "                                    \"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\n",
        "                                    \"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
        "\n",
        "g = sns.barplot(\"CrossValMeans\", \"Algorithm\", data=cv_res, palette=\"Set3\", orient=\"h\", **{\"xerr\":cv_std})\n",
        "g.set_xlabel(\"Mean Accuracy\")\n",
        "g = g.set_title(\"Cross Validation Socres\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldrBwtWdIkM"
      },
      "source": [
        "SVC, Adaboost, RandomForest, ExtraTrees and the GradientBoosting classifiers for the ensenble modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHrFCa4HdWat"
      },
      "source": [
        "### 6.1.2 Hyperparameter tunning for best models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbL9GEIAWuin"
      },
      "source": [
        "### META MODELING WITH ADABOOST, RF, EXTRATREES AND GRADIENTBOOSTING\n",
        "\n",
        "# Adaboost\n",
        "DTC = DecisionTreeClassifier()\n",
        "\n",
        "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
        "\n",
        "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
        "                  \"base_estimator__splitter\" : [\"best\", \"random\"],\n",
        "                  \"algorithm\" : [\"SAMME\", \"SAMME.R\"],\n",
        "                  \"n_estimators\" : [1,2],\n",
        "                  \"learning_rate\" : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 1.5]}\n",
        "\n",
        "gsadaDTC = GridSearchCV(adaDTC, param_grid=ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
        "\n",
        "gsadaDTC.fit(X_train, Y_train)\n",
        "\n",
        "ada_best = gsadaDTC.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-NBN6WOWudW"
      },
      "source": [
        "gsadaDTC.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lPEfggQWuaz"
      },
      "source": [
        "# ExtraTress\n",
        "\n",
        "ExtC = ExtraTreesClassifier()\n",
        "\n",
        "## Search grid for optimal parameters\n",
        "\n",
        "ex_param_grid = {\"max_depth\" : [None],\n",
        "                 \"max_features\" : [1, 3, 10],\n",
        "                 \"min_samples_split\" : [2,3,10],\n",
        "                 \"min_samples_leaf\" : [1,2,10],\n",
        "                 \"bootstrap\" : [False],\n",
        "                 \"n_estimators\" : [100,300],\n",
        "                 \"criterion\" : [\"gini\"]}\n",
        "\n",
        "gsExtC = GridSearchCV(ExtC, param_grid=ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
        "\n",
        "gsExtC.fit(X_train, Y_train)\n",
        "\n",
        "ExtC_best = gsExtC.best_estimator_\n",
        "\n",
        "# Best Socre\n",
        "gsExtC.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TPSc9_BWuYP"
      },
      "source": [
        "# RFC parameters tunning\n",
        "\n",
        "RFC = RandomForestClassifier()\n",
        "\n",
        "## Search grid for optimal parameters\n",
        "rf_param_grid = {\"max_depth\" : [None],\n",
        "                 \"max_features\" : [1,3,10],\n",
        "                 \"min_samples_split\" : [2,3,10],\n",
        "                 \"min_samples_leaf\" : [1,2,10],\n",
        "                 \"bootstrap\" : [False],\n",
        "                 \"n_estimators\" : [100,300],\n",
        "                 \"criterion\" : [\"gini\"]}\n",
        "\n",
        "gsRFC = GridSearchCV(RFC, param_grid=rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
        "\n",
        "gsRFC.fit(X_train,Y_train)\n",
        "\n",
        "RFC_best = gsRFC.best_estimator_\n",
        "\n",
        "# Best Score\n",
        "gsRFC.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdoVpwuKWuVr"
      },
      "source": [
        "# Gradient boosting tunning\n",
        "\n",
        "GBC = GradientBoostingClassifier()\n",
        "gb_param_grid = {\"loss\" : [\"deviance\"],\n",
        "                 \"n_estimators\" : [100,200,300],\n",
        "                 \"learning_rate\" : [0.1, 0.05, 0.01],\n",
        "                 \"max_depth\" : [4,8],\n",
        "                 \"min_samples_leaf\" : [100,150],\n",
        "                 \"max_features\" : [0.3, 0.1]}\n",
        "\n",
        "gsGBC = GridSearchCV(GBC, param_grid=gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
        "\n",
        "gsGBC.fit(X_train ,Y_train)\n",
        "\n",
        "GBC_best = gsGBC.best_estimator_\n",
        "\n",
        "# Best Score\n",
        "gsGBC.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrwzdUHOWuTI"
      },
      "source": [
        "### SVC classifier\n",
        "SVMC = SVC(probability=True)\n",
        "svc_param_grid = {'kernel': ['rbf'], \n",
        "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
        "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
        "\n",
        "gsSVMC = GridSearchCV(SVMC, param_grid=svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=4, verbose=1)\n",
        "\n",
        "gsSVMC.fit(X_train, Y_train)\n",
        "\n",
        "SVMC_best = gsSVMC.best_estimator_\n",
        "\n",
        "# Best score\n",
        "gsSVMC.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqdrd2UAnXko"
      },
      "source": [
        "### 6.1.3 Plot learning curves\n",
        "\n",
        "Learning curves are a good way to see the overfitting effect on the training set and the effect of the traing size on the accuacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCo29LV5WuQp"
      },
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "  \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
        "  plt.figure()\n",
        "  plt.title(title)\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.xlabel(\"Training examples\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "  train_scores_mean = np.mean(train_scores, axis=1)\n",
        "  train_scores_std = np.std(train_scores, axis=1)\n",
        "  test_scores_mean = np.mean(test_scores, axis=1)\n",
        "  test_scores_std = np.std(test_scores, axis=1)\n",
        "  plt.grid()\n",
        "\n",
        "  plt.fill_between(train_sizes, \n",
        "                   train_scores_mean - train_scores_std,\n",
        "                   train_scores_mean + train_scores_std,\n",
        "                   alpha=0.1,\n",
        "                   color=\"r\")\n",
        "  plt.fill_between(train_sizes, \n",
        "                   test_scores_mean - test_scores_std,\n",
        "                   test_scores_mean + test_scores_std,\n",
        "                   alpha=0.1,\n",
        "                   color=\"g\")\n",
        "  plt.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training Score\")  \n",
        "  plt.plot(train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-Validation Score\")\n",
        "\n",
        "  plt.legend(loc=\"best\")\n",
        "  return plt\n",
        "\n",
        "g = plot_learning_curve(gsRFC.best_estimator_, \"RF Learning Curves\", X_train, Y_train, cv=kfold)\n",
        "g = plot_learning_curve(gsExtC.best_estimator_, \"EctraTrees Learning Curves\", X_train, Y_train, cv=kfold)\n",
        "g = plot_learning_curve(gsSVMC.best_estimator_, \"SVC Learning Curves\", X_train, Y_train, cv=kfold)\n",
        "g = plot_learning_curve(gsadaDTC.best_estimator_, \"AdaBoost Learning Curves\", X_train, Y_train, cv=kfold)\n",
        "g = plot_learning_curve(gsGBC.best_estimator_, \"GradientBoosting Learning Curves\", X_train, Y_train, cv=kfold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jep5G4FIqPej"
      },
      "source": [
        "GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n",
        "\n",
        "SVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvnvKgvYqR87"
      },
      "source": [
        "### 6.1.4 Feature importance of tree based classifiers\n",
        "\n",
        "In order to see the most informative features for the prediction of passengers survival, displayed the feature importance for the 4 tree based classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P_dDXxjWuN5"
      },
      "source": [
        "nrows = ncols = 2\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=\"all\", figsize=(15,15))\n",
        "\n",
        "names_classifiers = [(\"AdaBoosting\", ada_best), (\"ExtraTress\", ExtC_best), (\"RandomForest\", RFC_best), (\"GradientBoostin\", GBC_best)]\n",
        "\n",
        "nclassifier = 0\n",
        "for row in range(nrows):\n",
        "  for col in range(ncols):\n",
        "    name = names_classifiers[nclassifier][0]\n",
        "    classifier = names_classifiers[nclassifier][1]\n",
        "    indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n",
        "    g = sns.barplot(y=X_train.columns[indices][:40], x=classifier.feature_importances_[indices][:40],\n",
        "                    orient=\"h\", ax=axes[row][col])\n",
        "    g.set_xlabel(\"Relative importance\", fontsize=12)\n",
        "    g.set_ylabel(\"Feature\", fontsize=12)\n",
        "    g.tick_params(labelsize=9)\n",
        "    g.set_title(name + \" featrue importance\")\n",
        "    nclassifier += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXPbmY-sspO"
      },
      "source": [
        "We note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example 'Fare', 'Title_2', 'Age' and 'Sex'.\n",
        "\n",
        "Title_2 which indicates the Mrs/Mlle/Mme/Miss/Ms category is highly correlated with Sex.\n",
        "\n",
        "We can say that:\n",
        "\n",
        "* Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers.\n",
        "* Sex and Title_2 (Mrs/Mlle/Mme/Miss/Ms) and Title_3 (Mr) refer to the gender.\n",
        "* Age and Title_1 (Master) refer to the age of passengers.\n",
        "* Fsize, LargeF, MedF, Single refer to the size of the passenger family.\n",
        "\n",
        "According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu1XPdf0WuK-"
      },
      "source": [
        "test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\n",
        "test_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\n",
        "test_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
        "test_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\n",
        "test_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n",
        "\n",
        "\n",
        "# Concatenate all classifier results\n",
        "ensemble_results = pd.concat([test_Survived_RFC,\n",
        "                              test_Survived_ExtC,\n",
        "                              test_Survived_AdaC,\n",
        "                              test_Survived_GBC, \n",
        "                              test_Survived_SVMC],\n",
        "                             axis=1)\n",
        "\n",
        "\n",
        "g= sns.heatmap(ensemble_results.corr(),annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN7BcqBQs-xC"
      },
      "source": [
        "The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers.\n",
        "\n",
        "The 5 classifiers give more or less the same prediction but there is some differences. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5iW-TLztCzs"
      },
      "source": [
        "## 6.2 Ensemble modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Zhlb9jtGpa"
      },
      "source": [
        "### 6.2.1 Combining models\n",
        "\n",
        "A Voting classifier to combine the predictions coming from 5 classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op8Wiud0WuIG"
      },
      "source": [
        "votingC = VotingClassifier(estimators=[(\"rfc\", RFC_best), (\"extc\", ExtC_best),\n",
        "                                       (\"svc\", SVMC_best), (\"adac\",ada_best),\n",
        "                                       (\"gbc\",GBC_best)], voting=\"soft\", n_jobs=4)\n",
        "\n",
        "votingC = votingC.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuxW6_cttitr"
      },
      "source": [
        "## 6.3 Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiIBLo4tlK-"
      },
      "source": [
        "6.3.1 Predict and Submit resutls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NAiFePdWuFh"
      },
      "source": [
        "test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n",
        "\n",
        "results = pd.concat([IDtest, test_Survived], axis=1)\n",
        "\n",
        "results.to_csv(\"ensemble_python_voting.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}