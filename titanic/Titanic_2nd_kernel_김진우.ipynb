{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic_2nd_kernel_김진우.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrjPVO7GE03v"
      },
      "source": [
        "필사 참조:\n",
        "\n",
        "https://www.kaggle.com/ash316/eda-to-prediction-dietanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEznJcBF-KXm"
      },
      "source": [
        "# Part1: Exploratory Data Analysis(EDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDYGq6g89ebN"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR7EI9It-WxC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVmpVkUh9-wq"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/titanic/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g38zUAE--UR6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMWJ_qwV-dp2"
      },
      "source": [
        "data.isna().sum() # checking for total null value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNJBjM--mPI"
      },
      "source": [
        "* The Age, Cabin and Embarked have null values. I will try to fix them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCFqQFgc-xMT"
      },
      "source": [
        "### ***How many Survived?*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6gA2KDn-jcZ"
      },
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(18,8))\n",
        "data[\"Survived\"].value_counts().plot.pie(explode=[0,0.1], autopct=\"%1.1f%%\", ax=ax[0], shadow=True)\n",
        "ax[0].set_title(\"Survived\")\n",
        "ax[0].set_ylabel(\"\")\n",
        "\n",
        "sns.countplot(\"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"Survived\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jnReVWcAh_3"
      },
      "source": [
        "It is evident that not many passengers survived the accident.\n",
        "\n",
        "Outof 891 passengers in traing set, only arount 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did surviv and who didn't.\n",
        "\n",
        "We will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port O Emabarcation, Age, etc.\n",
        "\n",
        "First let us understand the diffenrent types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxqkzs3wBWDD"
      },
      "source": [
        "## Types Of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeiZxOIvBliO"
      },
      "source": [
        "#### Categorical Features:\n",
        "A categorical variable is one that has Two or more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories(male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.\n",
        "\n",
        "**Categorical Features in the dataset : Sex, Embarked**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK2RwJRHCOAd"
      },
      "source": [
        "#### Ordinal Features:\n",
        "An ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: if we have a feature like **Height** with values **Tall, Medium, Short,** the Height is a ordinal variable. Here we can have a relative sort in the variable\n",
        "\n",
        "**Ordinal Features in the dataset : PClass**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHAT5GoCxrP"
      },
      "source": [
        "#### Countinous Features:\n",
        "A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n",
        "\n",
        "**Continous Features in the dataset : Age**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmPeYV_lDNrB"
      },
      "source": [
        "## Ananlysisng The Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STXeRcQyDQ_x"
      },
      "source": [
        "### Sex → Categorical Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G817Eh77_nL5"
      },
      "source": [
        "data.groupby([\"Sex\", \"Survived\"])[\"Survived\"].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2PskG2_nId"
      },
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(18,8))\n",
        "data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"]).mean().plot.bar(ax=ax[0])\n",
        "ax[0].set_title(\"Survived vs Sex\")\n",
        "\n",
        "sns.countplot(\"Sex\", hue=\"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"Sex:Survived vs Dead\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWLaWEFxEPdo"
      },
      "source": [
        "This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a **women on the ship is around 75% while that for men in arount 18-19%**.\n",
        "\n",
        "This looks to be a **very important feature** for modeling. But is it the best?? Lets check other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DkaJqxQE37a"
      },
      "source": [
        "### Pclass → Ordinal Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H7CJt5R_nF1"
      },
      "source": [
        "pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsPRU4W4_nDN"
      },
      "source": [
        "f, ax = plt.subplots(1,2, figsize=(18,8))\n",
        "data[\"Pclass\"].value_counts().plot.bar(color=[\"#CD7F32\",\"#FFDF00\",\"#D3D3D3\"], ax=ax[0])\n",
        "ax[0].set_title(\"Number Of Passengers By Pclass\")\n",
        "ax[0].set_ylabel(\"Count\")\n",
        "\n",
        "sns.countplot(\"Pclass\", hue=\"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"Pcalss:Survived vs Dead\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gljtx4KKIcrI"
      },
      "source": [
        "People say Money Can't Buy Everything. But we can clearly see that Passengers Of Pclass 1 were given a very Hig priority while rescue. Even though the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somwhere around 25%.\n",
        "\n",
        "For Pclass 1 survived is around 63% while for Pclass 2 is rount 48%. So money and status matters. Such a materialistic world.\n",
        "\n",
        "Lets Dive in little bit more and check for other interesting observations. Lets check survival rate with Sex and Pclass Together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpsl23Q_nAt"
      },
      "source": [
        "pd.crosstab([data.Sex, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6a-3p4_m9u"
      },
      "source": [
        "sns.factorplot(\"Pclass\", \"Survived\", hue=\"Sex\", data=data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlsF55uoJ1yq"
      },
      "source": [
        "We ues FactorPlot in this case, because the make the seperation of categorical values easy.\n",
        "\n",
        "Looking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pcalss 1 is about 95~96%, as only 3 out of 94 women from Pclass 1 died.\n",
        "\n",
        "It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men form Pclass 1 have a very low survival rate.\n",
        "\n",
        "Looks like Pclass is also an important feature. Lets analyse other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6KlqFzCunxm"
      },
      "source": [
        "### Age → Continous Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEOKoL8U_m7F"
      },
      "source": [
        "print(\"Oldest Passenger was of   :\", data.Age.max(), \"Years\")\n",
        "print(\"Youngest Passenger was of :\", data.Age.min(), \"Years\")\n",
        "print(\"Average Passenger was of  :\", data.Age.mean(), \"Years\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMk1pLLC_m4a"
      },
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(18,8))\n",
        "\n",
        "sns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[0])\n",
        "ax[0].set_title(\"Pclass and Age vs Survived\")\n",
        "ax[0].set_yticks(range(0, 110, 10))\n",
        "\n",
        "sns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=data, split=True, ax=ax[1])\n",
        "ax[1].set_title(\"Sex and Age vs Survived\")\n",
        "ax[1].set_yticks(range(0, 110, 10))\n",
        "plt.show()\n",
        "\n",
        "# split True 일 경우 한 개의 바이올린플롯에 Survived 0,1 둘다 표시\n",
        "# split False 일 경우 Survived 0, 1 각 표시"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXzlSst2wScT"
      },
      "source": [
        "#### Observations:\n",
        "1) The number of children increases with Pclass and the survival rate for passengers below Age10(1.e children) looks to be good irrespecive of the Pclass.\n",
        "\n",
        "2) Survival chances for Passengers anged 20-50 from Pclass1 is high and is even better for Women.\n",
        "\n",
        "3) For males, the survival chances decreases with an increase in age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHl-jllExIgF"
      },
      "source": [
        "As we had seen eariler, the Age feature has 177 null values. to replace these NaN values, we can assgin them the mean age of the dataset.\n",
        "\n",
        "But the problem is, there were many people with many different ages. We just can't assign a 4 year kid with the mean ages that is 29 years. Is there anay way to find out what age-band does the passenger lie?\n",
        "\n",
        "Bingo!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Tues we can assgin the mean values of Mr and Mrs to the respective groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEvCJCUd_mzW"
      },
      "source": [
        "data[\"Initial\"] = 0\n",
        "for i in data:\n",
        "  data[\"Initial\"] = data.Name.str.extract(\"([A-Za-z]+)\\.\") # lets extract the Salutataion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzA2-K3Ayi-E"
      },
      "source": [
        "Okay so here we are using the Rehex: **[A-Za-z]+)** .. So what it does is, it looks for strings which lie between ***A-Z or a-z*** and followed by a ***.(dot)***. So we successfully extract the Initials from the Name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL7WVlfC_mw5"
      },
      "source": [
        "pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap=\"summer_r\") #Checking the Initials with the Sex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHJOtGhUzWvK"
      },
      "source": [
        "Okay so there are some misspelled Initails like Mlle or Mme that stand for Miss. I will replcase them with Miss and same thing for other values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9FuKOl_muR"
      },
      "source": [
        "data[\"Initial\"].replace([\"Mlle\",\"Mme\",\"Ms\",\"Dr\",\"Major\",\"Lady\",\"Countess\",\"Jonkheer\",\"Col\",\"Rev\",\"Capt\",\"Sir\",\"Don\"],\n",
        "                        [\"Miss\",\"Miss\",\"Miss\",\"Mr\",\"Mr\",\"Mrs\",\"Mrs\",\"Other\",\"Other\",\"Other\",\"Mr\",\"Mr\",\"Mr\"],\n",
        "                        inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSeo7XBV_mr-"
      },
      "source": [
        "data.groupby(\"Initial\")[\"Age\"].mean() # lets check the average age by Initails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qskwkBQz4Zp"
      },
      "source": [
        "#### Filling Nan Ages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_OgaZnY_mo6"
      },
      "source": [
        "## Assigning the Nan Values with the Ceil values of the mean ages\n",
        "data.loc[(data.Age.isna()) & (data.Initial == \"Mr\"), \"Age\"] = 33\n",
        "data.loc[(data.Age.isna()) & (data.Initial == \"Mrs\"), \"Age\"] = 36\n",
        "data.loc[(data.Age.isna()) & (data.Initial == \"Master\"), \"Age\"] = 5\n",
        "data.loc[(data.Age.isna()) & (data.Initial == \"Miss\"), \"Age\"] = 22\n",
        "data.loc[(data.Age.isna()) & (data.Initial == \"Other\"), \"Age\"] = 46"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzfXeCp_mmZ"
      },
      "source": [
        "data.Age.isna().any() # So no null values left finally"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMXHuM0A_mj5"
      },
      "source": [
        "f, ax = plt.subplots(1,2, figsize=(20,10))\n",
        "data[data[\"Survived\"] == 0].Age.plot.hist(ax=ax[0], bins=20, edgecolor=\"black\", color=\"red\")\n",
        "ax[0].set_title(\"Survived=0\")\n",
        "x1 = list(range(0,85,5))\n",
        "ax[0].set_xticks(x1)\n",
        "\n",
        "data[data[\"Survived\"] == 1].Age.plot.hist(ax=ax[1], bins=20, edgecolor=\"black\", color=\"green\")\n",
        "ax[1].set_title(\"Survived=0\")\n",
        "x2 = list(range(0,85,5))\n",
        "ax[1].set_xticks(x2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuGyIXrO1I39"
      },
      "source": [
        "#### Observations:\n",
        "1) The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n",
        "\n",
        "2) The oldest Passengers was saved(80years).\n",
        "\n",
        "3) Maximum number of deaths were in the age group of 30-40."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyi1FpLW_mha"
      },
      "source": [
        "sns.factorplot(\"Pclass\", \"Survived\", col=\"Initial\", data=data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNr6PXQI16Ev"
      },
      "source": [
        "\bThe Women and Child first policy thus holds true ireespective of the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSoxpiAP6U3C"
      },
      "source": [
        "### Embared → Categorical Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhF2DwkX_meh"
      },
      "source": [
        "pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins=True).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGxU6eL06vIt"
      },
      "source": [
        "#### Chances for Survival by Port Of Embarkation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmTuuI4S_mcF"
      },
      "source": [
        "sns.factorplot(\"Embarked\", \"Survived\", data=data)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(5,3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71wT1W857Ewe"
      },
      "source": [
        "The chances for suvival for Port C is highest around 0.55 while it is lowest for S."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G5txHqk_mZm"
      },
      "source": [
        "f,ax = plt.subplots(2,2, figsize=(20,15))\n",
        "sns.countplot(\"Embarked\", data=data, ax=ax[0,0])\n",
        "ax[0,0].set_title(\"No. Of Passengers Boarded\")\n",
        "\n",
        "sns.countplot(\"Embarked\",hue=\"Sex\", data=data, ax=ax[0,1])\n",
        "ax[0,1].set_title(\"Male-Female Split for Embarked\")\n",
        "\n",
        "sns.countplot(\"Embarked\",hue=\"Survived\" ,data=data, ax=ax[1,0])\n",
        "ax[1,0].set_title(\"Embarked vs Survived\")\n",
        "\n",
        "sns.countplot(\"Embarked\",hue=\"Pclass\", data=data, ax=ax[1,1])\n",
        "ax[1,1].set_title(\"Embarked vs Pclass\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kRXbMB37b46"
      },
      "source": [
        "#### Observations:\n",
        "1) Maximum passengers boarded from S.Majority of them being from Pclass3.\n",
        "\n",
        "2) The passenges form C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pcalss1 and Pclass2 Passengers.\n",
        "\n",
        "3) The embark S looksto the port from where majority of the rich people boarded. Still the chances for surcical is low here, that is because many passengers from Pclass3 around 81% didn't survive.\n",
        "\n",
        "4) Port Q had almost 95% of the passengers were from Pclass3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqCJfZYV_mW9"
      },
      "source": [
        "sns.factorplot(\"Pclass\",\"Survived\",hue=\"Sex\",col=\"Embarked\",data=data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQoyLnuh8sFL"
      },
      "source": [
        "#### Observations:\n",
        "1) The survival chances are almost 1 from women for Pclass1 and Pclass2 irrespective of the Pclass.\n",
        "\n",
        "2) Port S looks to be very unlucky for Pclass3 Passengers as the survival rate for both men and women is very low.(Money Matters)\n",
        "\n",
        "3) Port Q looks to be unlukiest for men, as almost all were from Pclass3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh5dUs_V9NI5"
      },
      "source": [
        "#### Filling Embarked Nan\n",
        "As we saw that maximum Passengers boarded from Ports, we replace with S."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoAwmg0_mUj"
      },
      "source": [
        "data[\"Embarked\"].fillna(\"S\",inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJT6zoD5_mSC"
      },
      "source": [
        "data.Embarked.isnull().any() # Finally No NaN values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmDAnr429cPD"
      },
      "source": [
        "### SibSip → Discrete Feature\n",
        "This feature represents whether a person is alone or with his family members.\n",
        "\n",
        "Sibling = brother, sister, stepbrother, stepsister\n",
        "\n",
        "Spouse = husband, wife"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7D3IjE8_mPn"
      },
      "source": [
        "pd.crosstab(data.SibSp, data.Survived).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4SjwV0E_mM7"
      },
      "source": [
        "f, ax = plt.subplots(1,2, figsize=(20,8))\n",
        "sns.barplot(\"SibSp\", \"Survived\", data=data, ax=ax[0])\n",
        "ax[0].set_title(\"SibSp vs Survived\")\n",
        "\n",
        "\n",
        "sns.factorplot(\"SibSp\", \"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"SibSp vs Survived\")\n",
        "# plt.close(2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH3gifwD_mKp"
      },
      "source": [
        "pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389HsqfG_Wx-"
      },
      "source": [
        "#### Observations:\n",
        "The barplot and factorplot shows that if a passenger is alone onboard with no sibling, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That if, if I have a famaily on board, I will try to save them instead of saving myself first. Suprisingly the Survaval for families with 5-8 members is 0%. The reason may be Pclass??\n",
        "\n",
        "The reason is **Pclass.** The crosstab shows that Person SibSp>3 were all in Pclass3. It is imminent that all the families in Pclass3(>3) died."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt_AgzTaAwAJ"
      },
      "source": [
        "### Parch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2u7H6eM_mIO"
      },
      "source": [
        "pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2LppWJNA8Ix"
      },
      "source": [
        "The crosstab again shows that larger families were in Pclass3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJhM_6FU_mFr"
      },
      "source": [
        "f, ax = plt.subplots(1,2, figsize=(20,8))\n",
        "sns.barplot(\"Parch\", \"Survived\", data=data, ax=ax[0])\n",
        "ax[0].set_title(\"Parch vs Survived\")\n",
        "\n",
        "sns.factorplot(\"Parch\", \"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"Parch vs Survived\")\n",
        "\n",
        "# plt.close(2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ayp5K8hBjFK"
      },
      "source": [
        "#### Observations:\n",
        "Here too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n",
        "\n",
        "The chanced of survival is good for someboby who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when sombidy has >4 parents on the ship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k94ipm2gCggI"
      },
      "source": [
        "### Fare → Continous Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol49n4nK_mDS"
      },
      "source": [
        "print(\"Highest Fare was:\", data[\"Fare\"].max())\n",
        "print(\"Lowest Fare was:\", data[\"Fare\"].min())\n",
        "print(\"Average Fare was:\", data[\"Fare\"].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRsyjVgXCrPp"
      },
      "source": [
        "The lowest fare is 0.0. a free luxorious ride."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_WXONdU_mAy"
      },
      "source": [
        "f, ax = plt.subplots(1,3, figsize=(20,8))\n",
        "sns.distplot(data[data[\"Pclass\"] == 1].Fare, ax=ax[0])\n",
        "ax[0].set_title(\"Fares in Pcalss 1\")\n",
        "\n",
        "sns.distplot(data[data[\"Pclass\"] == 2].Fare, ax=ax[1])\n",
        "ax[1].set_title(\"Fares in Pcalss 2\")\n",
        "\n",
        "sns.distplot(data[data[\"Pclass\"] == 3].Fare, ax=ax[2])\n",
        "ax[2].set_title(\"Fares in Pcalss 3\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEBtbIMdDWgx"
      },
      "source": [
        "There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the stnadards reduces. As this is also continous, we can convert into discrete values by using binning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URx1CDqTD5KZ"
      },
      "source": [
        "### Observations in a Nutshell for all features:\n",
        "\n",
        "***Sex*** : The chance of survival for women is high as compared to men.\n",
        "\n",
        "***Pclass*** : There is a visible trend that being a **1st class passenger** gives you better chances of survival. The survival rate for*** Pclass3 is very low***. For ***women***, the chance of survival from ***Pclass1*** is almost 1 and is high too for those from Pclass2. Money Wins!\n",
        "\n",
        "***Age*** : Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n",
        "\n",
        "***Embared*** : This is a very intersting feature. ***The chances of survival at C looks to be better than even though the majority of Pclass1 Passengers got up at S.*** Pssgerners at Q were all from Pclass3.\n",
        "\n",
        "***Parch+SibSp*** : Having 1-2 siblings, spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a lager family travelling with you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xOqvo5F5zu"
      },
      "source": [
        "### Correlation Between The Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7o3amLw_l-g"
      },
      "source": [
        "sns.heatmap(data.corr(), annot=True, cmap=\"RdYlGn\", linewidths=0.2) # data.corr() → correlation matrix\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10,8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSMtbWMxHLar"
      },
      "source": [
        "#### Interpreting The Heatmap\n",
        "The first thing to note in that only numeric features are compared as it is obvious that we cannot correlate between alphabets or string. Before understanding the plot, let us see what exactly correlatin is.\n",
        "\n",
        "***POSITIVE CORRELATION*** : If an increase in feature A leads to increase in feature B, then they are positively correlated. A value ***1 means perfect positive correation.***\n",
        "\n",
        "***NEGATIVE CORRELATION*** : If an increase in feature A leads to decrease in feature B, then they are nagatively correlated. A value ***-1 means perfect negative correation.***\n",
        "\n",
        "Now lets say that two features are highy or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing ghighly similar informatrion and there is very little or no variance in information. This is known as ***MultiColinearity*** as both of them contains almost the same information.\n",
        "\n",
        "So do toy think we should use both of thme as ***one of them is redundant.*** While making or training models, we should try to eliminate redundant features as it reduces training time and my such advantages.\n",
        "\n",
        "Now from the above heatmap, wecan see that the featrues ar not much corrlated. The higest correlation is between ***SibSp and Parch i.e 0.41***. So We can carry on with all features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ujrJd4JlVd"
      },
      "source": [
        "# Part2 : Feature Engineering and Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eljLyon3N0Lm"
      },
      "source": [
        "Now what is Feature Engineering?\n",
        "\n",
        "Whenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n",
        "\n",
        "An example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gQC73C7N19J"
      },
      "source": [
        "### Age_band\n",
        "\n",
        "Problem With Age Feature:\n",
        "\n",
        "Age is a continous featrue, there is a problem with Continous Variables in Machine Learning Models.\n",
        "\n",
        "Eg : If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.\n",
        "\n",
        "Now if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\n",
        "\n",
        "We need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n",
        "\n",
        "Okay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16. So bins of size 16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsMRMk-W1jqx"
      },
      "source": [
        "data[\"Age_band\"] = 0\n",
        "data.loc[data[\"Age\"] <= 16,\"Age_band\"] = 0\n",
        "data.loc[(data[\"Age\"] > 16) & (data[\"Age\"] <= 32), \"Age_band\"] = 1\n",
        "data.loc[(data[\"Age\"] > 32) &(data[\"Age\"] <= 48), \"Age_band\"] = 2\n",
        "data.loc[(data[\"Age\"] > 48) &(data[\"Age\"] <= 64), \"Age_band\"] = 3\n",
        "data.loc[data[\"Age\"] > 64,\"Age_band\"] = 4\n",
        "data.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTVCqmfZ1joY"
      },
      "source": [
        "data[\"Age_band\"].value_counts().to_frame().style.background_gradient(cmap=\"summer\") # checking the number of passenegers in each band"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmvOhupC1jl9"
      },
      "source": [
        "sns.factorplot(\"Age_band\",\"Survived\", data=data,col=\"Pclass\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaunrxk4QkzL"
      },
      "source": [
        "True that.the survival rate decreases as the age increases irrespective of the Pclass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Z4k6UBQpAg"
      },
      "source": [
        "### Famaily_Size and Alone\n",
        "\n",
        "At this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can Check if survival rate have anything to do with family size of the passengers. Alone will denot whether a passenger is alone or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffPzuox11jjU"
      },
      "source": [
        "data[\"Family_Size\"] = 0\n",
        "data[\"Family_Size\"] = data[\"Parch\"] + data[\"SibSp\"] # family size\n",
        "data[\"Alone\"] = 0\n",
        "data.loc[data.Family_Size == 0, \"Alone\"] = 1 # alone\n",
        "\n",
        "f, ax = plt.subplots(1, 2, figsize=(18,6))\n",
        "sns.factorplot(\"Family_Size\", \"Survived\", data=data, ax=ax[0])\n",
        "ax[0].set_title(\"Family_Size vs Survived\")\n",
        "\n",
        "sns.factorplot(\"Alone\", \"Survived\", data=data, ax=ax[1])\n",
        "ax[1].set_title(\"Alone vs Survived\")\n",
        "\n",
        "plt.close(1)\n",
        "# plt.close(2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sykXjnUiSUN2"
      },
      "source": [
        "***Family_Size=0 means that the passeneger is alone.*** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRJURT51jg9"
      },
      "source": [
        "sns.factorplot(\"Alone\", \"Survived\", data=data, hue=\"Sex\", col=\"Pclass\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLG-wfLcSook"
      },
      "source": [
        "It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zfkTvb_SrtU"
      },
      "source": [
        "### Fare_Range\n",
        "\n",
        "Since fare is also a continous feature, we need to convert it into ordinal value. For this we will use ***pandas.qcut***.\n",
        "\n",
        "So what ***qcut*** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R68J5SPU1jeT"
      },
      "source": [
        "data[\"Fare_Range\"] = pd.qcut(data[\"Fare\"],4)\n",
        "data.groupby([\"Fare_Range\"])[\"Survived\"].mean().to_frame().style.background_gradient(cmap=\"summer_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65y7RN5QTALS"
      },
      "source": [
        "As discussed above, we can clearly see that as the ***fare_range increases, the chances of survival increases.***\n",
        "\n",
        "Now we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in ***Age_Band***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_-zpR7H1jb_"
      },
      "source": [
        "data[\"Fare_cat\"] = 0\n",
        "data.loc[data[\"Fare\"] <= 7.91, \"Fare_cat\"] = 0\n",
        "data.loc[(data[\"Fare\"] > 7.91) & (data[\"Fare\"] <= 14.454), \"Fare_cat\"] = 1\n",
        "data.loc[(data[\"Fare\"] > 14.454) & (data[\"Fare\"] <= 31), \"Fare_cat\"] = 2\n",
        "data.loc[(data[\"Fare\"] > 31) & (data[\"Fare\"] <= 513), \"Fare_cat\"] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXbCBty31jZl"
      },
      "source": [
        "sns.factorplot(\"Fare_cat\", \"Survived\", data=data, hue=\"Sex\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wd8tPIoUXSN"
      },
      "source": [
        "Cleary, as the Fare_cat increases, the survival chances increases. This feature may be become an important feature during modeling along with the Sex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7AMkM4HUqNq"
      },
      "source": [
        "### Converting String Values into Numeric\n",
        "\n",
        "Since we cannot pass strings to a machine learning model, we need to convert featrues Sex, Embarked, etc into numerinc values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMgTgTji1jXI"
      },
      "source": [
        "data[\"Sex\"].replace([\"male\", \"female\"], [0,1], inplace=True)\n",
        "data[\"Embarked\"].replace([\"S\", \"C\", \"Q\"], [0,1,2], inplace=True)\n",
        "data[\"Initial\"].replace([\"Mr\", \"Mrs\", \"Miss\", \"Master\", \"Other\"], [0,1,2,3,4], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlN5jfx1VZ9O"
      },
      "source": [
        "#### Dropping UnNeeded Features\n",
        "Name → We don't need name feature as it cannot be converted into any categorical value.\n",
        "\n",
        "Age → We have the Age_band feature, so no need of this.\n",
        "\n",
        "Ticket → It is any random string that cannot be categorised.\n",
        "\n",
        "Fare → We have the Fare_cat feature, so unneeded\n",
        "\n",
        "Cabin → A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n",
        "\n",
        "Fare_Range → We have the fare_cat feature.\n",
        "\n",
        "PassengerId → Cannot be categorised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUJ5eNYl1jUc"
      },
      "source": [
        "data.drop([\"Name\", \"Age\", \"Ticket\", \"Fare\", \"Cabin\", \"Fare_Range\", \"PassengerId\"], axis=1, inplace=True)\n",
        "sns.heatmap(data.corr(), annot=True, cmap=\"RdYlGn\", linewidths=0.2, annot_kws={\"size\":20})\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(18,15)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhn2_z0pWbwi"
      },
      "source": [
        "Now the above correlation plot, we can see some positively related features. Some of them being SibSp and Family_Size and Parch and Family_size and some negative ones like Alone and Family_Size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmETOxARWsHa"
      },
      "source": [
        "# Part3 : Predictive Modeling\n",
        "\n",
        "We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether ther Passenger will suvive or not using some great Classification Algorithms. Following are the algorithms I will use to make the model:\n",
        "\n",
        "1) Logistic Regression\n",
        "\n",
        "2) Support Vector Machines(Linear and radial)\n",
        "\n",
        "3) Random Forest\n",
        "\n",
        "4) K-Nearest Neighbours\n",
        "\n",
        "5) Navive Bayes\n",
        "\n",
        "6) Decision Tres\n",
        "\n",
        "7) Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQ4RuMO1jSD"
      },
      "source": [
        "# importing all the required ML packages\n",
        "from sklearn.linear_model import LogisticRegression # logistic regression\n",
        "from sklearn import svm # support vector Machine\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
        "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
        "from sklearn.naive_bayes import GaussianNB # Naive bayes\n",
        "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
        "from sklearn.model_selection import train_test_split # training and testing data split\n",
        "from sklearn import metrics # accuracy measure\n",
        "from sklearn.metrics import confusion_matrix # for confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiuRYj5L1jPe"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.3, random_state=0, stratify=data[\"Survived\"])\n",
        "train_X = train[train.columns[1:]]\n",
        "train_Y = train[train.columns[:1]]\n",
        "test_X = test[test.columns[1:]]\n",
        "test_Y = test[test.columns[:1]]\n",
        "X = data[data.columns[1:]]\n",
        "Y = data[\"Survived\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHh_EWEvYJQv"
      },
      "source": [
        "#### Radial Support Vector Machines(rdf-SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4duHOfBn1jNN"
      },
      "source": [
        "model = svm.SVC(kernel=\"rbf\", C=1, gamma=0.1)\n",
        "model.fit(train_X, train_Y)\n",
        "prediction1 = model.predict(test_X)\n",
        "print(\"Accuracy for rbf SVM is\", metrics.accuracy_score(prediction1, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UbZHp2vYsk5"
      },
      "source": [
        "#### Linear Support Vector Machine(linear-SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIXcDpis1jK7"
      },
      "source": [
        "model = svm.SVC(kernel=\"linear\", C=0.1, gamma=0.1)\n",
        "model.fit(train_X, train_Y)\n",
        "prediction2 = model.predict(test_X)\n",
        "print(\"Accuracy for linear SVM is\", metrics.accuracy_score(prediction2, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7476vKcZCbp"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_l0SwkC1jIr"
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(train_X, train_Y)\n",
        "prediction3 = model.predict(test_X)\n",
        "print(\"The accuracy of the Logistic Regression is\", metrics.accuracy_score(prediction3, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfaikoHZZZxc"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBvAp6kY1jGX"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(train_X, train_Y)\n",
        "prediction4 = model.predict(test_X)\n",
        "print(\"the accuracy of the Decision Tree is\", metrics.accuracy_score(prediction4, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJua1yjadte"
      },
      "source": [
        "#### K-Nearest Neighbours(KNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZwkmvhe1jBv"
      },
      "source": [
        "model = KNeighborsClassifier()\n",
        "model.fit(train_X, train_Y)\n",
        "prediction5 = model.predict(test_X)\n",
        "print(\"The accuracy of the KNN is\", metrics.accuracy_score(prediction5, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27CMTtuCbY5O"
      },
      "source": [
        "Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default value is 5. Lets check the accuracies over various values of n_neighbours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P296SrUY1i9R"
      },
      "source": [
        "a_index = list(range(1,11))\n",
        "a = pd.Series()\n",
        "x = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "for i in list(range(1,11)):\n",
        "  model = KNeighborsClassifier(n_neighbors=i)\n",
        "  model.fit(train_X, train_Y)\n",
        "  prediction = model.predict(test_X)\n",
        "  a = a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\n",
        "\n",
        "plt.plot(a_index, a)\n",
        "plt.xticks(x)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(12,6)\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy for different values of n are:\", a.values, \"with the max value as\", a.values.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NFt4OGacbHc"
      },
      "source": [
        "#### Gaussian Navie Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8lLETzf1i7A"
      },
      "source": [
        "model = GaussianNB()\n",
        "model.fit(train_X, train_Y)\n",
        "prediction6 = model.predict(test_X)\n",
        "print(\"The accuracy of the NaiveBayes is\", metrics.accuracy_score(prediction6, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTNFeR99czJb"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdAtTviC1i4p"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(train_X, train_Y)\n",
        "prediction7 = model.predict(test_X)\n",
        "print(\"The accuracy of the Random Forests is\", metrics.accuracy_score(prediction7, test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccwOSdcdeSb"
      },
      "source": [
        "The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n",
        "\n",
        "Now this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over?? The answer is ***No***, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as*** model variance***.\n",
        "\n",
        "To overcome this and get a generalized model,we use ***Cross Validation.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev-_xAOyXhoA"
      },
      "source": [
        "## Cross Validataion\n",
        "\n",
        "Many a times, the data is imbalanced, i.e there may be a high number of class1 intances but less number of other class instances. Thus we should train and test outl algorithm on each and every instance of the dataset. Then we can take an average of all the noted acuuracies ober the dataset.\n",
        "\n",
        "1) The K-Fold Cross Validation works by fist dividing the dataset into k-subsets.\n",
        "\n",
        "2) Let's say we divide the dataset into (k=5) parts. We reserve 1 part for thesting and train the algorithm over the 4 parts.\n",
        "\n",
        "3) We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The auuracies and errors are then avertaged to get a average auuracy of the algorithm.\n",
        "\n",
        "This is called K-Fold Corss Validation.\n",
        "\n",
        "4) An alorithm amy underfit over a dataset for som training data and sometimes also overfit the data for other traininig set. Thus with cross-validation, we can achieve a generallised model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hbNrEJ1i1g"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
        "\n",
        "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
        "\n",
        "xyz = []\n",
        "accuracy = []\n",
        "std = []\n",
        "\n",
        "classifiers = [\"Linear Svm\", \"Radial Svm\", \"Logistic Regression\", \"KNN\", \n",
        "               \"Decision Tree\", \"Navie Bayes\", \"Random Forest\"]\n",
        "models = [svm.SVC(kernel=\"linear\"), svm.SVC(kernel=\"rbf\"), LogisticRegression(), KNeighborsClassifier(n_neighbors=9),\n",
        "          DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]\n",
        "\n",
        "for i in models:\n",
        "  model = i\n",
        "  cv_result = cross_val_score(model, X, Y, cv=kfold, scoring=\"accuracy\")\n",
        "  cv_result  = cv_result\n",
        "  xyz.append(cv_result.mean())\n",
        "  std.append(cv_result.std())\n",
        "  accuracy.append(cv_result)\n",
        "\n",
        "new_model_dataframe2 = pd.DataFrame({\"CV Mean\" : xyz, \"Std\" : std}, index=classifiers)\n",
        "new_model_dataframe2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ-NEuME1iyo"
      },
      "source": [
        "plt.subplots(figsize=(12,6))\n",
        "box = pd.DataFrame(accuracy, index=[classifiers])\n",
        "box.T.boxplot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc-5DPQx1iwA"
      },
      "source": [
        "new_model_dataframe2[\"CV Mean\"].plot.barh(width=0.8)\n",
        "plt.title(\"Average CV Mean Accuracy\")\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(8,5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZy3RKaKbewF"
      },
      "source": [
        "The classfication accuracy can be sometimes misleading due to imbalaced. We can get a summarized resul with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Rda1lIb4kn"
      },
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "It gives the number of correct and incorrect classfication made by the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2P2y5JXYREj"
      },
      "source": [
        "f, ax = plt.subplots(3,3, figsize=(12,10))\n",
        "\n",
        "y_pred = cross_val_predict(svm.SVC(kernel=\"rbf\"), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,0], annot=True, fmt=\"2.0f\")\n",
        "ax[0,0].set_title(\"Matrix for rbf-SVM\")\n",
        "\n",
        "y_pred = cross_val_predict(svm.SVC(kernel=\"linear\"), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,1], annot=True, fmt=\"2.0f\")\n",
        "ax[0,1].set_title(\"Matrix for Linear-SVM\")\n",
        "\n",
        "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,2], annot=True, fmt=\"2.0f\")\n",
        "ax[0,2].set_title(\"Matrix for KNN\")\n",
        "\n",
        "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,0], annot=True, fmt=\"2.0f\")\n",
        "ax[1,0].set_title(\"Matrix for Random-Forests\")\n",
        "\n",
        "y_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,1], annot=True, fmt=\"2.0f\")\n",
        "ax[1,1].set_title(\"Matrix for Logistic Regression\")\n",
        "\n",
        "y_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,2], annot=True, fmt=\"2.0f\")\n",
        "ax[1,2].set_title(\"Matrix for Decision Tress\")\n",
        "\n",
        "y_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[2,0], annot=True, fmt=\"2.0f\")\n",
        "ax[2,0].set_title(\"Matrix for GaussianNB\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKWzbhizd3rA"
      },
      "source": [
        "#### Interpreting Confusion Matrix\n",
        "\n",
        "The left diahonal shows the number of correct prediction made for each calss while the right diagonal shows the number fo wrong prediction made. Lets consider the first plot for rbf-SVM:\n",
        "\n",
        "1) The no.of correct predictions are 491(for dead) + 247(for survived) with the mean SV accuracy being (491+247)/891 = 82.8% which we did get eariler.\n",
        "\n",
        "2) Errors → Wrongly Classfied 58 dead people as survived and 94 survived as dead. Thus it has made more mistakes by predictiong dead as survived.\n",
        "\n",
        "By looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passngers but NaiveBayes has a higher chance in correctly predicting passengers who survivied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCgzqYDOfb7A"
      },
      "source": [
        "#### Hyper-Parameters Tuning\n",
        "\n",
        "The machine learning models are like a Black-Box. There are som default parameter values for this Black-Box, which we can tun ofr change to get a better model. Like the C and gamma in the SVM model and simialy different paramethers for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the alogorithm and get a better model. This is known as Hyper-Parameter Tuning.\n",
        "\n",
        "We will tune the hyper-parameters for the 2 best clssifiers i.e the SVM and RandomForests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUpHyGOYgLg9"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSwO_JWKYRAq"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "C = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "gamma = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\n",
        "kernel = [\"rbf\", \"linear\"]\n",
        "hyper = {\"kernel\" : kernel, \"C\" : C, \"gamma\" : gamma}\n",
        "gd = GridSearchCV(estimator=svm.SVC(), param_grid=hyper, cv=3, verbose=True)\n",
        "gd.fit(X, Y)\n",
        "\n",
        "print(gd.best_score_)\n",
        "print(gd.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPeseVXphV8c"
      },
      "source": [
        "#### Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE6ZS4JGYQ41"
      },
      "source": [
        "n_estimators = range(100, 1000, 100)\n",
        "hyper = {\"n_estimators\" : n_estimators}\n",
        "\n",
        "gd = GridSearchCV(estimator=RandomForestClassifier(random_state=0), cv=3, param_grid=hyper, verbose=True)\n",
        "gd.fit(X, Y)\n",
        "\n",
        "print(gd.best_score_)\n",
        "print(gd.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3pU7OtAidor"
      },
      "source": [
        "The Best score for ***Rbf-Svm is 82.82% with C=0.5 and gamma=0.2***. For ***RandomForest score is about 81.82% with n_estimators=900***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_y4aPcVi8j1"
      },
      "source": [
        "# Ensembling\n",
        "\n",
        "Ensembling is a good waty to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to vreate a single powerful model.\n",
        "\n",
        "lets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single produvt after analysing all different parameters. This in Ensembling, which improves the stability of the model. Ensembling can be done in ways like:\n",
        "\n",
        "1) Voting Classifier\n",
        "\n",
        "2) Bagging\n",
        "\n",
        "3) Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I_sGRkXkZE1"
      },
      "source": [
        "### Voting Classifier\n",
        "\n",
        "It is the simplest way of combining predictions from may different simple machine learning models. It gives an agerage prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of different types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckVXrJrMYQuy"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble_lin_rbf = VotingClassifier(estimators=[(\"KNN\", KNeighborsClassifier(n_neighbors=10)),\n",
        "                                                (\"RBF\", svm.SVC(probability=True, kernel=\"rbf\", C=0.5, gamma=0.1)),\n",
        "                                                (\"RFor\", RandomForestClassifier(n_estimators=900, random_state=0)),\n",
        "                                                (\"LR\", LogisticRegression(C=0.05)),\n",
        "                                                (\"DT\", DecisionTreeClassifier(random_state=0)),\n",
        "                                                (\"NB\", GaussianNB()),\n",
        "                                                (\"svm\", svm.SVC(kernel=\"linear\", probability=True))],\n",
        "                                    voting=\"soft\").fit(train_X, train_Y)\n",
        "\n",
        "print(\"The accuracy for ensembled model is:\", ensemble_lin_rbf.score(test_X, test_Y))\n",
        "\n",
        "cross = cross_val_score(ensemble_lin_rbf, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated score is\", cross.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w44XQtBbmjZH"
      },
      "source": [
        "### Bagging\n",
        "\n",
        "Bagging is a general ensemble method. It works by applying simiar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging, there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bZ5rqtdnCrl"
      },
      "source": [
        "### Bagged KNN\n",
        "Bagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCxXIhBHYQq3"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "model = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=0, n_estimators=700)\n",
        "model.fit(train_X, train_Y)\n",
        "prediction = model.predict(test_X)\n",
        "print(\"The accuracy for bagged KNN is:\", metrics.accuracy_score(prediction, test_Y))\n",
        "\n",
        "result = cross_val_score(model, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated socre fro bagged KNN is:\", result.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np2HaeMU1Tsf"
      },
      "source": [
        "### Baged Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiuW_Lhj1S05"
      },
      "source": [
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=0, n_estimators=100)\n",
        "model.fit(train_X, train_Y)\n",
        "prediction = model.predict(test_X)\n",
        "print(\"The accuracy for bagged Decision Tree is:\", metrics.accuracy_score(prediction, test_Y))\n",
        "result = cross_val_score(model, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated score for bagged Decision Tree is:\", result.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BbnCUyeoO9w"
      },
      "source": [
        "### Boosting\n",
        "\n",
        "Boosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model. Boosting works as follows:\n",
        "\n",
        "A model is first trained on the complete dataset. Now the model will get som instances right while som wrong. Now in the next iteration, the learner will forcus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifiers are added to model until the limit is reached on the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv49y5IFqZOO"
      },
      "source": [
        "#### AdaBooust(Adaptive Boosting)\n",
        "\n",
        "The weak learner or estimator in this case is a Decsion Tree. But we can change the default base_estimator to any algorithm of our choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYEOopHZYQm9"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.1)\n",
        "result = cross_val_score(ada, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated score for AdaBoost is:\", result.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E3Chy_rrADb"
      },
      "source": [
        "#### Stochastic Gradient Boosting\n",
        "\n",
        "Here too the weak learner is a Descision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjlLFfUZYQi_"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "grad = GradientBoostingClassifier(n_estimators=500, random_state=0, learning_rate=0.1)\n",
        "result = cross_val_score(grad, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated score for Gradient Boosting is:\", result.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev1rXuKsrh3s"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqC3wfc7YQfD"
      },
      "source": [
        "import xgboost as xg\n",
        "\n",
        "xgboost = xg.XGBClassifier(n_estmators=900, learning_rate=0.1)\n",
        "result = cross_val_score(xgboost, X, Y, cv=10, scoring=\"accuracy\")\n",
        "print(\"The cross validated score for XGBoost is:\", result.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ZkL6dPr5PD"
      },
      "source": [
        "#### Hyper-Parameter Tuning for Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0NSMlfeYQaO"
      },
      "source": [
        "n_estimators = list(range(100,1100,100))\n",
        "learn_rate = [0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
        "hyper = {\"n_estimators\":n_estimators, \"learning_rate\":learn_rate}\n",
        "gd = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyper, cv=3, verbose=True)\n",
        "gd.fit(X,Y)\n",
        "print(gd.best_score_)\n",
        "print(gd.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEt9DaBJsJRI"
      },
      "source": [
        "The maximum accuracy we can get with AdaBoost is ***83.16% wiht n_estimators=200 and learning_rage = 0.05***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyoAI19Psr9n"
      },
      "source": [
        "### Counfusion Matrix for the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP3MyiKzYQVS"
      },
      "source": [
        "ada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.05)\n",
        "result = cross_val_predict(ada, X, Y, cv=10)\n",
        "sns.heatmap(confusion_matrix(Y, result), cmap=\"winter\", annot=True, fmt=\"2.0f\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG171anRtJr-"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUSPRYqYQRX"
      },
      "source": [
        "f, ax = plt.subplots(2,2, figsize=(15,12))\n",
        "model = RandomForestClassifier(n_estimators=500, random_state=0)\n",
        "model.fit(X,Y)\n",
        "pd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0,0])\n",
        "ax[0,0].set_title(\"Feature Importance in Random Forests\")\n",
        "\n",
        "model=AdaBoostClassifier(n_estimators=200, learning_rate=0.05, random_state=0)\n",
        "model.fit(X,Y)\n",
        "pd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0,1], color=\"#ddff11\")\n",
        "ax[0,1].set_title(\"Feature Importance in AdaBoost\")\n",
        "\n",
        "model=GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, random_state=0)\n",
        "model.fit(X,Y)\n",
        "pd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1,0], cmap=\"RdYlGn_r\")\n",
        "ax[1,0].set_title(\"Feature Importance in Gradient Boosting\")\n",
        "\n",
        "model=xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\n",
        "model.fit(X,Y)\n",
        "pd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1,1],color=\"#FD0F00\")\n",
        "ax[1,1].set_title(\"Feature Importance in XgBoost\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBc3DmdHtu0Z"
      },
      "source": [
        "We can see the important features for various clssifiers like RandomForest, AdaBoost, etc\n",
        "\n",
        "#### Observation:\n",
        "\n",
        "1) Some of the common important feature are Initail, Fare_cat, Pclass, Family_size.\n",
        "\n",
        "2) The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier thet Sex combined with Pclass was giving a very good differetiating factor. Sex looks to be important only in Random Forest.\n",
        "\n",
        "However, we can see the featrue Initail, which is at the top in many classifiers. We had already seen the positive correlation between Sex and Initail, so the both refer to the gender.\n",
        "\n",
        "3) Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone, Parch and SibSp."
      ]
    }
  ]
}