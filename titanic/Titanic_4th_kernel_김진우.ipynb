{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic_4th_kernel_김진우.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKMimMTWB88THW3+PGc0JP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinwooxxi/kagglestudy/blob/main/titanic/Titanic_4th_kernel_%EA%B9%80%EC%A7%84%EC%9A%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajgj3HcOH_1l"
      },
      "source": [
        "필사 참조:\n",
        "\n",
        "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVKWUdPMvMSB"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n",
        "\n",
        "The Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning Kaggle competitions there seems to be a dearth of kernels on this topic so I hope this notebook can fill somewhat of that void."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t03GlgBruFm0"
      },
      "source": [
        "# Load in our libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Going to use these 5 base models for the stacking\n",
        "from sklearn.ensemble import (RandomForestClassifier, \n",
        "                              AdaBoostClassifier, \n",
        "                              GradientBoostingClassifier,\n",
        "                              ExtraTreesClassifier)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFyiqg4mwCht"
      },
      "source": [
        "# Feature Exploration, Engineering and Clearing\n",
        "\n",
        "Now we will proceed much like how most kernels in generall are structured, and that in to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorial features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCmE_xZAvhFi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShT_HRMmwdR-"
      },
      "source": [
        "# Load in the train and test dataset\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/titanic/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/titanic/test.csv\")\n",
        "\n",
        "# Store our passenger ID for easy access\n",
        "PassengerId = test[\"PassengerId\"]\n",
        "\n",
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C6Fmq8FwzAP"
      },
      "source": [
        "Well it is no surprice that our task is to somehow extract the information out of the categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCGgLWYnw_wx"
      },
      "source": [
        "#### Feature Engineering\n",
        "\n",
        "Here, credit must be extended to Sina's very comprehensive and well-thouth out notebook for the feature engineering ideas so please check out his work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcj1eCxBwepn"
      },
      "source": [
        "full_data = [train, test]\n",
        "\n",
        "# Some features of my own that I have added in\n",
        "# Gives the length of the name\n",
        "train[\"Name_length\"] = train[\"Name\"].apply(len)\n",
        "test[\"Name_length\"] = test[\"Name\"].apply(len)\n",
        "\n",
        "# Feature that tells whether a passenger had a cabin on th Titinic\n",
        "train[\"Has_Cabin\"] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
        "test[\"Has_Cabin\"] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
        "\n",
        "# Feature engineering steps taken from Sina\n",
        "# Create new feature FamailySize as a combination of SibSp and Parch\n",
        "for dataset in full_data:\n",
        "  dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n",
        "\n",
        "# Create new feature IsAlone from FamilySize\n",
        "for dataset in full_data:\n",
        "  dataset[\"IsAlone\"] = 0\n",
        "  dataset.loc[dataset[\"FamilySize\"] ==1, \"IsAlone\"] = 1\n",
        "\n",
        "# Remove all NULLS in the Embarked col\n",
        "for dataset in full_data:\n",
        "  dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\n",
        "\n",
        "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
        "for dataset in full_data:\n",
        "  dataset[\"Fare\"] = dataset[\"Fare\"].fillna(train[\"Fare\"].median())\n",
        "train[\"CategoricalFare\"] = pd.qcut(train[\"Fare\"], 4)\n",
        "\n",
        "# Create a New feature CategoficalAge\n",
        "for dataset in full_data:\n",
        "  age_avg = dataset[\"Age\"].mean()\n",
        "  age_std = dataset[\"Age\"].std()\n",
        "  age_null_count = dataset[\"Age\"].isna().sum()\n",
        "  age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
        "  dataset[\"Age\"][np.isnan(dataset[\"Age\"])] = age_null_random_list\n",
        "  dataset[\"Age\"] = dataset[\"Age\"].astype(int)\n",
        "train[\"CategoricalAge\"] = pd.cut(train[\"Age\"], 5)\n",
        "\n",
        "# Define function to extract titles from passenger names\n",
        "def get_title(name):\n",
        "  title_search = re.search(\" ([A-Za-z]+)\\.\", name)\n",
        "  # If the title exists, extract and return it.\n",
        "  if title_search:\n",
        "    return title_search.group(1)\n",
        "  return \"\"\n",
        "\n",
        "# Create a new feature Title, containing the titles of passenger names\n",
        "for dataset in full_data:\n",
        "  dataset[\"Title\"] = dataset[\"Name\"].apply(get_title)\n",
        "\n",
        "# Group all non-common titles into one single grouping \"Rare\"\n",
        "for dataset in full_data:\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].replace([\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Dr\",\n",
        "                                              \"Major\", \"Don\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mlle\", \"Miss\")\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].replace(\"Ms\", \"Miss\")\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mme\", \"Mrs\")\n",
        "\n",
        "for dataset in full_data:\n",
        "  # Mapping Sex\n",
        "  dataset[\"Sex\"] = dataset[\"Sex\"].map({\"female\":0, \"male\":1}).astype(int)\n",
        "  # Mapping Titles\n",
        "  title_mapping = {\"Mr\":1, \"Miss\":2, \"Mrs\":3, \"Master\":4, \"Rare\":5}\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].map(title_mapping)\n",
        "  dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\n",
        "  # Mapping Embarked\n",
        "  dataset[\"Embarked\"] = dataset[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\":2})\n",
        "  # Mapping Fare\n",
        "  dataset.loc[dataset[\"Fare\"] <=7.91, \"Fare\"] = 0\n",
        "  dataset.loc[(dataset[\"Fare\"] >7.91) % (dataset[\"Fare\"] <=14.454), \"Fare\"] = 1\n",
        "  dataset.loc[(dataset[\"Fare\"] >14.454) % (dataset[\"Fare\"] <=31), \"Fare\"] = 2\n",
        "  dataset.loc[dataset[\"Fare\"] >31, \"Fare\"] = 3\n",
        "  dataset[\"Fare\"] = dataset[\"Fare\"].astype(int)\n",
        "  # Mapping Age\n",
        "  dataset.loc[ dataset[\"Age\"] <= 16, \"Age\"] = 0\n",
        "  dataset.loc[(dataset[\"Age\"] > 16) & (dataset[\"Age\"] <= 32), \"Age\"] = 1\n",
        "  dataset.loc[(dataset[\"Age\"] > 32) & (dataset[\"Age\"] <= 48), \"Age\"] = 2\n",
        "  dataset.loc[(dataset[\"Age\"] > 48) & (dataset[\"Age\"] <= 64), \"Age\"] = 3\n",
        "  dataset.loc[ dataset[\"Age\"] > 64, \"Age\"] = 4 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nU_2bHWweni"
      },
      "source": [
        "# Feature Seletion\n",
        "drop_elements = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"SibSp\"]\n",
        "train = train.drop(drop_elements, axis=1)\n",
        "train = train.drop([\"CategoricalAge\", \"CategoricalFare\"], axis=1)\n",
        "test = test.drop(drop_elements, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DkvEK1J4Pxk"
      },
      "source": [
        "All right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGqSkKUp4Zj2"
      },
      "source": [
        "### Viualisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuyZmo6bwekR"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Fxx0Bd4d_-"
      },
      "source": [
        "#### Pearson Correlation Heatmap\n",
        "Let us generate som corrlation plots of the features to see how related on feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSNYPGm7wehn"
      },
      "source": [
        "colormap = plt.cm.RdBu\n",
        "plt.figure(figsize=(14,12))\n",
        "plt.title(\"Pearson Correlation of Featurs\", y=1.05, size=15)\n",
        "sns.heatmap(train.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor=\"white\", annot=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95U3SpB1JCle"
      },
      "source": [
        "#### Takeaway from the Plots\n",
        "\n",
        "OneThing that the Pearson Corrlation plot can tess us is that there are not too many features strongly correlated with on another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it som unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LXO6fMhJujd"
      },
      "source": [
        "#### Pairplots\n",
        "\n",
        "Finally let us generate som pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th63EZMLwee-"
      },
      "source": [
        "g = sns.pairplot(train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]], \n",
        "                 hue=\"Survived\", palette=\"seismic\", size=1.2, \n",
        "                 diag_kind=\"kde\", diag_kws=dict(shade=True), plot_kws=dict(s=10))\n",
        "g.set(xticklabels=[]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeNLMxZJMX2d"
      },
      "source": [
        "# Ensembling & Stacking models\n",
        "\n",
        "Finally after that brief whirlwind detour with regards to feature enginnering and formatting, we finally arrive at the meat and gist of the this notbook.\n",
        "\n",
        "Creating a Stacking ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3TH_iHoTH78"
      },
      "source": [
        "#### Helpers via Python Classes\n",
        "Here we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n",
        "\n",
        "In the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlxAaA0kweXO"
      },
      "source": [
        "# Some useful parameters which will come in handy later on\n",
        "ntrain = train.shape[0]\n",
        "ntest = test.shape[0]\n",
        "SEED = 0 # for reproducibility\n",
        "NFOLDS = 5 # set folds for out-of-fold prediction\n",
        "kf = KFold(n_splits=NFOLDS, random_state=SEED)\n",
        "\n",
        "# Class to extend the Skleran classifier\n",
        "\n",
        "class SklearnHelper(object):\n",
        "  def __init__(self, clf, seed=0, params=None):\n",
        "    params[\"random_state\"] = seed\n",
        "    self.clf = clf(**params)\n",
        "\n",
        "  def train(self, x_train, y_train):\n",
        "    self.clf.fit(x_train, y_train)\n",
        "  \n",
        "  def predict(self, x):\n",
        "    return self.clf.predict(x)\n",
        "  \n",
        "  def fit(self,x,y):\n",
        "    return self.clf.fit(x,y)\n",
        "  \n",
        "  def feature_importances(self,x,y):\n",
        "    print(self.clf.fit(x,y).feature_importances_)\n",
        "\n",
        "# Class to extend XGboost classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2SlJfNOOS6p"
      },
      "source": [
        "Bear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n",
        "\n",
        "def init : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n",
        "\n",
        "The rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Essentially, we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xbgPKWlOUhB"
      },
      "source": [
        "#### Out-of-Fold Predictions\n",
        "\n",
        "Now as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on th full training data, generate predictions on the full test set and then ouput these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these pridictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVyXQ3b3weVF"
      },
      "source": [
        "def get_oof(clf, x_train, y_train, x_test):\n",
        "  oof_train = np.zeros((ntrain,))\n",
        "  oof_test = np.zeros((ntest,))\n",
        "  oof_test_skf = np.empty((NFOLDS, ntest))\n",
        "\n",
        "  for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
        "    x_tr = x_train[train_index]\n",
        "    y_tr = y_train[train_index]\n",
        "    x_te = x_train[test_index]\n",
        "\n",
        "    clf.train(x_tr, y_tr)\n",
        "\n",
        "    oof_train[test_index] = clf.predict(x_te)\n",
        "    oof_test_skf[i, :] = clf.predict(x_test)\n",
        "\n",
        "  oof_test[:] = oof_test_skf.mean(axis=0)\n",
        "  return oof_train.reshape(-1,1), oof_test.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuvV8ZYnQBge"
      },
      "source": [
        "# Generating our Base First-Level Models\n",
        "\n",
        "So now let us prepare five learning models as out first level classification. Theses models can all be conveniently invoke via the Sklearn library and are listed as follows:\n",
        " \n",
        "\n",
        "1.   Random Forest classifier\n",
        "2.   Extra Trees classifier\n",
        "3. AdaBoost classifier\n",
        "4. Gradient Boosting classifier\n",
        "5. Support Vector Machine\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sks-XwrsQmv8"
      },
      "source": [
        "##### Parameters\n",
        "\n",
        "***n_jobs*** : Number of cores used for the training process. If set to -1, all cores are used.\n",
        "\n",
        "***n_estimators*** : Number of classification trees in your learning model(set to 10 per default)\n",
        "\n",
        "***max_depth*** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one woul be growing the tree too deep\n",
        "\n",
        "***verbose*** : Controls whether you want to outpput any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdGiushBweSK"
      },
      "source": [
        "# Put in our parameters for said classifiers\n",
        "# Random Forest parameters\n",
        "rf_params = {\"n_jobs\":-1,\n",
        "             \"n_estimators\":500,\n",
        "             \"warm_start\":True,\n",
        "             # \"max_features\":0.2,\n",
        "             \"max_depth\":6,\n",
        "             \"min_samples_leaf\":2,\n",
        "             \"max_features\":\"sqrt\",\n",
        "             \"verbose\":0}\n",
        "\n",
        "# Extra Trees parameters\n",
        "et_params = {\"n_jobs\":-1,\n",
        "             \"n_estimators\":500,\n",
        "             # \"max_features\":0.5,\n",
        "             \"max_depth\":8,\n",
        "             \"min_samples_leaf\":2,\n",
        "             \"verbose\":0}\n",
        "\n",
        "# AdaBoost paramters\n",
        "ada_params = {\"n_estimators\":500,\n",
        "              \"learning_rate\":0.75}\n",
        "\n",
        "# Gradient Boosting parameters\n",
        "gb_params = {\"n_estimators\":500,\n",
        "             # \"max_features\":0.2,\n",
        "             \"max_depth\":5,\n",
        "             \"min_samples_leaf\":2,\n",
        "             \"verbose\":0}\n",
        "\n",
        "# Support Vector Classifier parameters\n",
        "svc_params = {\"kernel\":\"linear\",\n",
        "              \"C\":0.025}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjsjpTO5S2h_"
      },
      "source": [
        "Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3IaEqgLwePw"
      },
      "source": [
        "# Create 5 objects that represent our 4 models\n",
        "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
        "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
        "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
        "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
        "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8v5maFnVxRq"
      },
      "source": [
        "#### Creating NumPy arrays out of our train and test sets\n",
        "\n",
        "Having prepared our first layer base models as such, we can now ready the training and test data for input into our classifiers by henerating NumPy arrays out of their original dataframes as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vCXP6JpweM_"
      },
      "source": [
        "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
        "y_train = train[\"Survived\"].ravel()\n",
        "train = train.drop([\"Survived\"], axis=1)\n",
        "x_train = train.values # Creates an array of the train data\n",
        "x_test = test.values # Creats an array of the test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1U0t_NWVcF"
      },
      "source": [
        "#### Output of the First level Predictions\n",
        "\n",
        "We now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c03vt4IrweKs"
      },
      "source": [
        "# Create our OOF train and test predictions. These base results will be used as new features\n",
        "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
        "rf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test) # Random Forest\n",
        "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
        "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
        "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
        "\n",
        "print(\"Training is complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cit1PUC8eNAJ"
      },
      "source": [
        "Feature importances generated from the different classifiers\n",
        "\n",
        "Now having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n",
        "\n",
        "As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .featureimportances. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al9-dcEMweIC"
      },
      "source": [
        "rf_feature = rf.feature_importances(x_train, y_train)\n",
        "et_feature = et.feature_importances(x_train, y_train)\n",
        "ada_feature = ada.feature_importances(x_train, y_train)\n",
        "gb_feature = gb.feature_importances(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV6LHtmBkW8p"
      },
      "source": [
        "rf_features = [0.12699105, 0.19637586, 0.02925355, 0.02033595, 0.07163159, 0.02257005,\n",
        "               0.10887744, 0.06877873, 0.06536339, 0.01587601, 0.27394637]\n",
        "et_features = [0.12538595, 0.38588855, 0.02705886, 0.01577156, 0.03991573, 0.03005334,\n",
        "               0.04706786, 0.08597022, 0.04412663, 0.02558009, 0.17318122]\n",
        "ada_features = [0.01, 0.01, 0.014, 0.038, 0.384, 0.018, 0.418, 0.008, 0.032, 0.006, 0.062]\n",
        "gb_features = [0.07944947, 0.00874461, 0.04207254, 0.00831864, 0.11391303, 0.01936122,\n",
        "               0.15885507, 0.03406444, 0.1070065, 0.00716802, 0.42104645]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fVBZEIzhO8b"
      },
      "source": [
        "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-LSVzIdweFn"
      },
      "source": [
        "cols = train.columns.values\n",
        "# Create a dataframe with featurs\n",
        "feature_dataframe = pd.DataFrame({\"features\":cols,\n",
        "                                  \"Random Forest feature importances\":rf_features,\n",
        "                                  \"Extra Trees feature importances\":et_features,\n",
        "                                  \"AdaBoost feature importances\":ada_features,\n",
        "                                  \"Gradient Boost feature importances\":gb_features})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPVfIf5Kh0jq"
      },
      "source": [
        "Interactive feature importances via Plotly scatterplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3E_sdNlweDJ"
      },
      "source": [
        "# Scatter plot\n",
        "trace = go.Scatter(y=feature_dataframe[\"Random Forest feature importances\"].values,\n",
        "                   x=feature_dataframe[\"features\"].values,\n",
        "                   mode=\"markers\",\n",
        "                   marker=dict(sizemode=\"diameter\",\n",
        "                               sizeref=1,\n",
        "                               size=25,\n",
        "                               color=feature_dataframe[\"Random Forest feature importances\"].values,\n",
        "                               colorscale=\"Portland\",\n",
        "                               showscale=True),\n",
        "                   text = feature_dataframe[\"features\"].values)\n",
        "data = [trace]\n",
        "\n",
        "layout = go.Layout(autosize=True,\n",
        "                   title=\"Random Forest Feature Importance\",\n",
        "                   hovermode=\"closest\",\n",
        "                   yaxis=dict(title=\"Feature Importance\",\n",
        "                              ticklen=5,\n",
        "                              gridwidth=2),\n",
        "                   showlegend=False)\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig,filename='scatter2010')\n",
        "\n",
        "# Scatter plot\n",
        "trace = go.Scatter(y=feature_dataframe[\"Extra Trees feature importances\"].values,\n",
        "                   x=feature_dataframe[\"features\"].values,\n",
        "                   mode=\"markers\",\n",
        "                   marker=dict(sizemode=\"diameter\",\n",
        "                               sizeref=1,\n",
        "                               size=25,\n",
        "                               color=feature_dataframe[\"Extra Trees feature importances\"].values,\n",
        "                               colorscale=\"Portland\",\n",
        "                               showscale=True),\n",
        "                   text = feature_dataframe[\"features\"].values)\n",
        "data = [trace]\n",
        "\n",
        "layout = go.Layout(autosize=True,\n",
        "                   title=\"Extra Trees feature importances\",\n",
        "                   hovermode=\"closest\",\n",
        "                   yaxis=dict(title=\"Feature Importance\",\n",
        "                              ticklen=5,\n",
        "                              gridwidth=2),\n",
        "                   showlegend=False)\n",
        "                   \n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig,filename='scatter2010')\n",
        "\n",
        "# Scatter plot\n",
        "trace = go.Scatter(y=feature_dataframe[\"AdaBoost feature importances\"].values,\n",
        "                   x=feature_dataframe[\"features\"].values,\n",
        "                   mode=\"markers\",\n",
        "                   marker=dict(sizemode=\"diameter\",\n",
        "                               sizeref=1,\n",
        "                               size=25,\n",
        "                               color=feature_dataframe[\"AdaBoost feature importances\"].values,\n",
        "                               colorscale=\"Portland\",\n",
        "                               showscale=True),\n",
        "                   text = feature_dataframe[\"features\"].values)\n",
        "data = [trace]\n",
        "\n",
        "layout = go.Layout(autosize=True,\n",
        "                   title=\"AdaBoost Feature Importance\",\n",
        "                   hovermode=\"closest\",\n",
        "                   yaxis=dict(title=\"Feature Importance\",\n",
        "                              ticklen=5,\n",
        "                              gridwidth=2),\n",
        "                   showlegend=False)\n",
        "                   \n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig,filename='scatter2010')\n",
        "\n",
        "# Scatter plot\n",
        "trace = go.Scatter(y=feature_dataframe[\"Gradient Boost feature importances\"].values,\n",
        "                   x=feature_dataframe[\"features\"].values,\n",
        "                   mode=\"markers\",\n",
        "                   marker=dict(sizemode=\"diameter\",\n",
        "                               sizeref=1,\n",
        "                               size=25,\n",
        "                               color=feature_dataframe[\"Gradient Boost feature importances\"].values,\n",
        "                               colorscale=\"Portland\",\n",
        "                               showscale=True),\n",
        "                   text = feature_dataframe[\"features\"].values)\n",
        "data = [trace]\n",
        "\n",
        "layout = go.Layout(autosize=True,\n",
        "                   title=\"Gradient Boost Feature Importance\",\n",
        "                   hovermode=\"closest\",\n",
        "                   yaxis=dict(title=\"Feature Importance\",\n",
        "                              ticklen=5,\n",
        "                              gridwidth=2),\n",
        "                   showlegend=False)\n",
        "                   \n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig,filename='scatter2010')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qti213IBwd-N"
      },
      "source": [
        "# Create the new column containing the averate of values\n",
        "\n",
        "feature_dataframe[\"mean\"] = feature_dataframe.mean(axis=1)\n",
        "feature_dataframe.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQBeZCdBnyJp"
      },
      "source": [
        "Plotly Barplot of Average Feature Importances\n",
        "\n",
        "Having obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3aqMwBMwd7v"
      },
      "source": [
        "y = feature_dataframe[\"mean\"].values\n",
        "x = feature_dataframe[\"features\"].values\n",
        "\n",
        "data = [go.Bar(x=x,\n",
        "               y=y,\n",
        "               width=0.5,\n",
        "               marker=dict(color=feature_dataframe[\"mean\"].values,\n",
        "                           colorscale=\"Portland\",\n",
        "                           showscale=True,\n",
        "                           reversescale=False),\n",
        "               opacity=0.6)]\n",
        "\n",
        "layout = go.Layout(autosize=True,\n",
        "                   title=\"Barplots of Mean Featuer Importance\",\n",
        "                   hovermode=\"closest\",\n",
        "                   yaxis=dict(title=\"Feature Importance\",\n",
        "                              ticklen=5,\n",
        "                              gridwidth=2),\n",
        "                   showlegend=False)\n",
        "\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig, filename=\"bar-direct-labels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejKtILwDpKOu"
      },
      "source": [
        "#  Second-Level Prediction from the First-level Output\n",
        "\n",
        "First-level output as new features\n",
        "\n",
        "Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqWNNPQlwd5Y"
      },
      "source": [
        "base_predictions_train = pd.DataFrame({\"RandomForest\": rf_oof_train.ravel(),\n",
        "                                       \"ExtraTrees\": et_oof_train.ravel(),\n",
        "                                       \"AdaBoost\": ada_oof_train.ravel(),\n",
        "                                       \"GradientBoost\": gb_oof_train.ravel()})\n",
        "base_predictions_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6DqGjdwd2p"
      },
      "source": [
        "data = [go.Heatmap(z=base_predictions_train.astype(float).corr().values,\n",
        "                   x=base_predictions_train.columns.values,\n",
        "                   y=base_predictions_train.columns.values,\n",
        "                   colorscale='Viridis',\n",
        "                   showscale=True,\n",
        "                   reversescale = True)]\n",
        "                   \n",
        "py.iplot(data, filename='labelled-heatmap')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNY089Iwd0X"
      },
      "source": [
        "x_train = np.concatenate((et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
        "x_test = np.concatenate((et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Gqkr-opuKm"
      },
      "source": [
        "####Second level learning model via XGBoost\n",
        "Here we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the official documentation.\n",
        "\n",
        "Anyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyyrppWlwdyD"
      },
      "source": [
        "gbm = xgb.XGBClassifier(#learning_rate = 0.02,\n",
        "                        n_estimators= 2000,\n",
        "                        max_depth= 4,\n",
        "                        min_child_weight= 2,\n",
        "                        #gamma=1,\n",
        "                        gamma=0.9,                        \n",
        "                        subsample=0.8,\n",
        "                        colsample_bytree=0.8,\n",
        "                        objective= 'binary:logistic',\n",
        "                        nthread= -1,\n",
        "                        scale_pos_weight=1).fit(x_train, y_train)\n",
        "                        \n",
        "predictions = gbm.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}